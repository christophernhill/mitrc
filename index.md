This page provides a summary list and contact links for some key computing and storage hardware (physical and virtual) resources that are available to support campus research. 

## MIT cloud credits program

| Name          | URL           | Details     |
| ------------- |:-------------:| ------------|
| Microsoft Azure Cloud   | [https://cloud.mit.edu/credits](https://cloud.mit.edu/credits) | The cloud credits program currently has Microsoft Azure credits available. These can be used for any of the Azure Virtual machines and Azure machine learning tools. The resources include clusters of recent generation Volta GPUs with high-speed interconnects, suitable for large machine learning workflows. |
|               |                               |  |
| Google Compute Platform | [https://cloud.mit.edu/credits](https://cloud.mit.edu/credits) | Currently all credits for Google Compute Platform have been assigned to projects. |


## MIT campus wide hardware

| Name          | URL           | Details     |
| ------------- |:-------------:| ------------|
| Engaging    | [https://engaging-web.mit.edu](https://engaging-web.mit.edu/eofe-wiki/) | The Engaging cluster is open to everyone on campus. It has around 80,000 x86 CPU cores and 300 GPU cards ranging from K80 generation to recent Voltas. Hardware access is through the Slurm resource scehduler that suports batch and interactive workloads and allows dedicated reservations. A wide range of standard software is available and the Docker compatible Singularity container tool is supported. A standard, web-based portal supporting Jupyter notebooks, R studio, Mathematica and X graphics is available at [https://engaging-ood.mit.edu](https://engaging-ood.mit.edu). Further information and support is available from [engaging-support@techsquare.com](mailto:engaging-support@techsquare.com). 
| | | |
| C3DDB | [https://c3ddb01.mit.edu/request_account](https://c3ddb01.mit.edu/request_account) | The C3DDB cluster is open to everyone on campus for research in the general area of life-sciences, health-sciences, computational biology, biochemistry and biomechanics. It has around 8000 x86 CPU cores and 100 K80 generation GPU cards. Hardware access is through the Slurm resource scheduler that suports batch and interactive workload and allows dedicated reservations. A wide range of standard software is available and the Docker compatible Singularity container tool is supported. Further information and support is available from [c3ddb-admin@techsquare.com](mailto:c3ddb-admin@techsquare.com).  |
| | | |
| Supercloud | [https://supercloud.mit.edu](https://supercloud.mit.edu) | The Supercloud system is a collaboration with MIT Lincoln Laboratory on a shared facility that is optimized for streamlining open research collaborations with Lincoln Laboratory. The facility is open to everyone on campus. The latest Supercloud system has more than 16,000 x86 CPU cores and more than 850 NVidia Volta GPUs in total. Hardware access is through the Slurm resource scehduler that suports batch and interactive workload and allows dedicated reservations. A wide range of standard software is available and the Docker compatible Singularity container tool is supported. A custom, web-based portal supporting Jupyter notebooks is available at [https://txe1-portal.mit.edu/](https://txe1-portal.mit.edu/). Further inforamtion and support is available at [supercloud@mit.edu](mailto:supercloud@mit.edu).
| | | |
| Satori | [https://mit-satori.github.io](https://mit-satori.github.io) | Satori is a IBM Power 9 large memory node system. It is open to evryone on campus and has optimized software stacks for machine learning and for image stack post-processing for MIT.nano Cryo-EM faciltiies. The system has 256 NVidia Volta GPU cards attached in groups of four to 1TB memory nodes and a total of 2560 Power 9 CPU cores. Hardware access is through the Slurm resource scehduler that suports batch and interactive workload and allows dedicated reservations. A wide range of standard software is available and the Docker compatible Singularity container tool is supported. A standard web based portal [https://satori-portal.mit.edu](https://satori-portal.mit.edu) with Jupyter notebook support is available. Further information and support is available at [satori-support@techsquare.com](mailto:satori-support@techsquare.com).
| | | |
| submit.mit.edu | [http://submit.mit.edu](http://submit.mit.edu) | submit.mit.edu is an experimental gateway to the Open Science Grid operated by the MIT Laboratory for Nuclear Science. It is intended to be available for anyone on campus. 

## DLC shared hardware

| Name          | URL           | Details     |
| ------------- |:-------------:| ------------|
| TIG Shared Computing          | [https://tig.csail.mit.edu/shared-computing](https://tig.csail.mit.edu/shared-computing) | The CSAIL infrastructure group (TIG) operates an Openstack cluster and a Slurm cluster for general use by members of CSAIL. The Openstack environment supports full virtual machines. The Slurm cluster supports Singularity as a container enginer for Docker containers. Furhter information and support is available at [help@csail.mit.edu](mailto:help@csail.mit.edu).
| | |
| Openmind | [https://openmind.mit.edu](https://openmind.mit.edu)| Openmind is a shared cluster for Brain and Cognitive Science research at MIT. It has several hundred GPU cards and just under 2000 CPU cores. The cluster resources are managed by the Slurm scheduler wich provides support for batch, interactive and reservation based use. The Singulairty container system is available for executing custom Docker images. Further information and support is available from [neuro-admin@techsquare.com](mailto:neuro-admin@techsquare.com).
| | |
| LNS Computing | [http://rc.lns.mit.edu](http://rc.lns.mit.edu) | The Laboratory for Nuclear Science in Physics operates computing resources that are available to researchers within LNS.  Further information and support is available from [pra@mit.edu](mailto:pra@mit.edu).
| | |
| Kavli Computing | [Kavli Computing](https://space.mit.edu/research/high-performance-computing/) | The MIT Kavli Institute operates a cluster for astrophysics research. THe cluster uses the Slurm resource scheduler and is available for use by Kavli researchers.
| | |
| Koch Bioinformatics | [https://ki.mit.edu/sbc/bioinformatics](https://ki.mit.edu/sbc/bioinformatics) | Koch operates a bioinformatics facility which specializes in processing needs of computational biologists
| | |

## COVID-19 research support programs

There are a multiple programs providing accelerated access to resources to COVID-19 related activities, some of these are

| Name          | URL           | Details     |
| ------------- |:-------------:| ------------|
| OSTP HPC COVID-19 | [https://covid19-hpc-consortium.org](https://covid19-hpc-consortium.org) | Provides rapid access to large compute resources for favorably reviewed mini-proposals. Resources available span very large supercomputers to commercial cloud providers. Projects in need of at scale computer resources for any work related to managing and mitigating the COVID-19 out break are elegible.
| | |
| MGHPCC COVID-19 | [https://www.mghpcc.org/mghpcc-resources-for-covid-19-research/](https://www.mghpcc.org/mghpcc-resources-for-covid-19-research) | University participants in the Masschusetts Green High Performance Computing Center are pooling capacity to provide accelerated access to COVID-19 projects that may need resources.
| | |
| AWS | [https://aws.amazon.com/data-exchange/covid-19](https://aws.amazon.com/data-exchange/covid-19) | AWS has assembled an open repository of COVID-19 data.
| | |
| Azure | [https://ai4hcovidgrants.microsoft.com](https://ai4hcovidgrants.microsoft.com) | The Microsoft Azure organization is providing grants theough its AI for Health program to support COVID-19 computational needs.
| | |
| IBM | [https://developer.ibm.com/callforcode/](https://developer.ibm.com/callforcode/) | IBM is sponsoring a call for code program for COVID-19 projects.
| | |
| GCP | [https://edu.google.com/programs/credits/research](https://edu.google.com/programs/credits/research) | Google cloud is providing a rapid cloud credits application process for COVID-19 related activities.
| | |


## Major cloud provider standard credits programs

| Provider      | URL           | Details     |
| ------------- |:-------------:| ------------|
|AWS  | https://aws.amazon.com/grants | Provides cloud credit grants for both research and education projects.
| | |
|AWS  | https://aws.amazon.com/opendata | Hosts open data for sharing with others. AWS has a process for applying to have datasets considered for hosting. 
| | |
| Azure | https://azure.microsoft.com/en-us/education | Provides Azure cloud credits for education
| | |
| Azure | https://azure.microsfoft.com/en-us/services/open-datasets | Hosts standard datasets for general use including machine learning. Additional datasets for inclusion can be nominated.
| | |
| Google | https://edu.google.com/programs/students, https://edu.google.com/programs/faculty, https://edu.google.com/programs/researchers | Offers free credits and technical resources for education and research.
| | |


## Long term storage

| Provider      | URL           | Details     |
| ------------- |:-------------:| ------------|
| Google        |   https://drive.google.com            | All MIT accounts include access to Google drive storage that
does not have any pre-set capacity limits. Files can be uploaded using web-clients or command line clients such as [rclone](https://rclone.org).
| | |
| Code42 | | 
| | |
| TSM | |
| NESE | |


## National facilities

 XSEDE, DOE Incite, Open Science Grid
 
 
### About this page

This paage is maintained in github. We welcome updates and addition pull requests. 
